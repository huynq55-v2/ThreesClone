Threes! Clone + N-Tuple AI ğŸ§ 

An accurate Android recreation of the classic puzzle game Threes!, featuring an embedded Reinforcement Learning AI (N-Tuple Network) that evaluates moves in real-time and learns from gameplay history.
ğŸ“‹ Overview

This project is not just a game clone; it is an experiment in On-Device Reinforcement Learning.

It ports the exact gameplay mechanics (Deck-based spawning, move logic) from the original Threes! source code to Java. On top of that, it integrates an N-Tuple Networkâ€”a lightweight, high-speed value network that predicts the long-term potential of the board state.

The core philosophy:

    Do you play well? Good, the AI will learn from your high scores. Do you play poorly? No problem. The AI learns from ground truth (game scores), so it learns to avoid your mistakes rather than repeating them.

âœ¨ Key Features
ğŸ® 1:1 Game Logic Replica

    Exact Mechanics: Implements the specific "shift-one-step" movement unique to Threes! (unlike 2048).

    True Randomness: Replicates the "Deck" (or Bag) spawning algorithm, ensuring the next tile distribution matches the original game's balance.

ğŸ§  Embedded N-Tuple AI

    Real-time Evaluation: Displays an "AI Eval" score (Potential) for every board state.

        <span style="color:green">Green:</span> High potential (Good state).

        <span style="color:red">Red:</span> Low potential (Risk of Game Over).

    Lightweight Architecture: Uses N-Tuple Networks (Lookup Tables), allowing thousands of evaluations per second on a mobile CPU without overheating.

ğŸ“ Training Mechanisms

    Self-Learning (On-Policy): The AI learns from the game history using Monte Carlo Backward Reward Calculation. It propagates the final game score back to previous moves to determine their true value.

    Knowledge Distillation: Supports importing training data (.txt logs) generated by powerful PPO agents on PC. The mobile AI "watches" these master plays and updates its weights to match high-level strategies (e.g., reaching 6144 tile).

ğŸ’¾ Persistence

    Save/Load Brain: The AI model (brain.dat) is saved locally and persists across app restarts.

    Continuous Improvement: The more you play (or feed it data), the smarter it gets.

ğŸ›  Technical Architecture
The N-Tuple Network

Instead of a heavy Convolutional Neural Network (CNN), this project uses N-Tuple Networks.

    Input: The 4x4 Board.

    Structure: The board is sliced into multiple tuples (rows, columns, 2x2 squares).

    Learning: Each tuple pattern maps to a weight in a Lookup Table. The board's value is the sum of these weights.

    Update Rule:
    V(s)â†V(s)+Î±(Gtâ€‹âˆ’V(s))

Reward Calculation (Backward Pass)

To handle the "delayed reward" problem, the game calculates the Discounted Return (Gtâ€‹) backwards from the end of the episode:
Gtâ€‹=Rtâ€‹+Î³Gt+1â€‹

Where:

    Rtâ€‹: Immediate score gained at step t.

    Î³: Discount factor (e.g., 0.99).

This ensures that a move earning 0 points early on can still be valued highly if it leads to a massive score later.
ğŸš€ Installation & Setup

    Clone the repository:
    Bash

    git clone https://github.com/huynq55-v2/ThreesClone.git

    Open in Android Studio:

        Select File > Open and choose the project directory.

        Let Gradle sync.

    Build & Run:

        Connect your Android device or use an Emulator.

        Run app (Minimum SDK: 24).

ğŸ•¹ Usage
Playing the Game

    Swipe to move tiles.

    Observe the "AI Eval" text to see how the AI rates your current board.

    Game Over: Tap the "GAME OVER" text to trigger the training process on the match you just finished.

Training from Logs (Knowledge Distillation)

To train the mobile AI using data from a PC agent (e.g., PPO):

    Generate a log file on PC (Format: 1,2,3...16_values|target_G).

    Place the file in the device storage (or implement a loader in MainActivity).

    The game parses the log and updates the N-Tuple weights via brain.train().

ğŸ“‚ Project Structure

com.example.threesclone
â”œâ”€â”€ Game.java           # Core game logic, Move history, RL Training loop
â”œâ”€â”€ NTupleNetwork.java  # AI Architecture (Lookup tables, predict, train)
â”œâ”€â”€ MainActivity.java   # UI, Gesture handling, Haptics/Audio
â”œâ”€â”€ Tile.java           # Tile object & Rank calculation
â”œâ”€â”€ Direction.java      # Enum for UP, DOWN, LEFT, RIGHT
â””â”€â”€ PseudoList.java     # Helper for Deck-based Randomness

ğŸ¤ Contributing

Contributions are welcome! If you have ideas for better tuple shapes, optimized training parameters, or UI improvements, feel free to open an Issue or Pull Request.
ğŸ“œ License

This project is an educational clone and is not affiliated with Sirvo (creators of the original Threes!). This code is provided for research and learning purposes.
